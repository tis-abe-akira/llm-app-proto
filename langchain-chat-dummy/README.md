# LangChain Chat Dummy Application

React + Vite + TypeScript フロントエンド と uv + FastAPI バックエンドを使ったストリーミング対応のダミーチャットアプリケーションです。

**注意: これはダミー実装です。実際のLLM（OpenAI GPT等）は呼び出されず、事前に定義された固定の応答を返します。**

## 機能

- ✅ ストリーミングレスポンスによるリアルタイム表示（ダミー）
- ✅ 会話メモリー機能（履歴保存） 
- ✅ LocalStorage による会話履歴の永続化
- ✅ サイドバーからの履歴管理
- ✅ 固定応答による動作テスト
- ✅ LLM API不要でのテスト環境構築

### 動作イメージ

このアプリケーションは実際のLLMを使用せず、事前に定義された日本語の応答をストリーミング形式で返します。

## セットアップ

### 1. バックエンド

```bash
cd backend
uv sync
# .env ファイルは不要です（ダミー実装のため）
```

### 2. フロントエンド  

```bash
cd frontend
npm install
```

## 実行

### バックエンドを起動

```bash
cd backend

# start-server.sh のプロキシを設定したうえで実行！
./start-server.sh
```

### フロントエンドを起動

```bash
cd frontend  
npm run dev
```

アプリは <http://localhost:5173> で利用できます。

## 設定

このダミー実装では特別な設定は不要です。
OpenAI API キーなどの外部サービス設定は必要ありません。

## ダミー応答について

このアプリケーションは以下のような固定応答を順番に返します：
- こんにちは！これはダミーの応答です。実際のLLMは呼び出されていません。
- お疲れ様です！固定の応答をお返ししています。
- ダミーチャットサービスが正常に動作しています。
- これはテスト用の固定メッセージです。LLM APIは使用していません。
- シンプルなダミー応答をストリーミング形式でお届けしています。

また、特定のキーワードに対しては専用の応答を返します：
- 「こんにちは」「hello」→ 挨拶メッセージ
- 「ありがとう」「thank」→ お礼メッセージ
- 「?」「？」を含む→ 質問への対応メッセージ
